"""
NotebookLM Enterprise API ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ì „ ìš”êµ¬ ì‚¬í•­:
1. Google Cloud í”„ë¡œì íŠ¸ ìƒì„±
2. NotebookLM API í™œì„±í™”
3. ì„œë¹„ìŠ¤ ê³„ì • ìƒì„± ë° í‚¤ ë‹¤ìš´ë¡œë“œ
4. í™˜ê²½ ë³€ìˆ˜: GOOGLE_APPLICATION_CREDENTIALS

ì‚¬ìš©ë²•:
    python notebooklm_upload.py --project-id YOUR_PROJECT_ID
"""

import os
import json
import argparse
from pathlib import Path

# Google Cloud í´ë¼ì´ì–¸íŠ¸ëŠ” í™˜ê²½ì— ë”°ë¼ ì„¤ì¹˜ í•„ìš”
# pip install google-cloud-notebooklm (Enterprise API í´ë¼ì´ì–¸íŠ¸)

# í˜„ì¬ Enterprise APIê°€ ê³µê°œëì§€ë§Œ í´ë¼ì´ì–¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ 
# ì•„ì§ ê³µì‹ PyPIì— ì—†ì„ ìˆ˜ ìˆìŒ. REST API ì§ì ‘ í˜¸ì¶œ í•„ìš”í•  ìˆ˜ ìˆìŒ.

SOURCE_PACK_DIR = Path(__file__).parent.parent.parent / "data" / "source_packs" / "bong"


def load_layer_files():
    """Load all layer files from the source pack directory."""
    layers = {}
    
    # Layer 0: Raw evidence
    layer0_path = SOURCE_PACK_DIR / "layer0_raw" / "shot_analysis_chunks.json"
    if layer0_path.exists():
        with open(layer0_path, "r", encoding="utf-8") as f:
            layers["layer0_raw"] = json.load(f)
    
    # Layer 1: Structured knowledge
    layer1_path = SOURCE_PACK_DIR / "layer1_structured" / "logic_persona_vectors.json"
    if layer1_path.exists():
        with open(layer1_path, "r", encoding="utf-8") as f:
            layers["layer1_structured"] = json.load(f)
    
    # Layer 2: Synthesized guides
    layer2_path = SOURCE_PACK_DIR / "layer2_synthesized" / "variation_guide_ko.md"
    if layer2_path.exists():
        with open(layer2_path, "r", encoding="utf-8") as f:
            layers["layer2_synthesized"] = f.read()
    
    return layers


def convert_to_notebooklm_format(layers: dict) -> list:
    """
    Convert layer files to NotebookLM source format.
    
    NotebookLM sources can be:
    - Google Docs/Slides (via URL)
    - Web URLs
    - Raw text
    - PDF (file upload)
    """
    sources = []
    
    # Layer 0: JSONì„ í¬ë§·ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if "layer0_raw" in layers:
        layer0_text = format_layer0_as_text(layers["layer0_raw"])
        sources.append({
            "type": "raw_text",
            "title": "ë´‰ì¤€í˜¸ í´ëŸ¬ìŠ¤í„° - Layer 0: Raw Evidence",
            "content": layer0_text,
            "metadata": {
                "layer": "layer0_raw",
                "cluster_id": "CL_BONG_01"
            }
        })
    
    # Layer 1: JSONì„ í¬ë§·ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if "layer1_structured" in layers:
        layer1_text = format_layer1_as_text(layers["layer1_structured"])
        sources.append({
            "type": "raw_text",
            "title": "ë´‰ì¤€í˜¸ í´ëŸ¬ìŠ¤í„° - Layer 1: Logic & Persona Vectors",
            "content": layer1_text,
            "metadata": {
                "layer": "layer1_structured",
                "cluster_id": "CL_BONG_01"
            }
        })
    
    # Layer 2: Markdownì€ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
    if "layer2_synthesized" in layers:
        sources.append({
            "type": "raw_text",
            "title": "ë´‰ì¤€í˜¸ í´ëŸ¬ìŠ¤í„° - Layer 2: ì˜¤ë§ˆì£¼ ë³€ì£¼ ê°€ì´ë“œ",
            "content": layers["layer2_synthesized"],
            "metadata": {
                "layer": "layer2_synthesized",
                "cluster_id": "CL_BONG_01"
            }
        })
    
    return sources


def format_layer0_as_text(data: dict) -> str:
    """Format Layer 0 JSON as readable text for NotebookLM."""
    lines = []
    lines.append("# ë´‰ì¤€í˜¸ í´ëŸ¬ìŠ¤í„° - ì›ë³¸ ì˜ìƒ ë¶„ì„ ê²°ê³¼ (Layer 0)")
    lines.append("")
    lines.append(f"í´ëŸ¬ìŠ¤í„° ID: {data.get('cluster_id', 'N/A')}")
    lines.append(f"ê°ë…: {data.get('auteur', 'N/A')}")
    lines.append(f"ìƒì„±ì¼: {data.get('generated_at', 'N/A')}")
    lines.append("")
    
    # Chunks
    chunks = data.get("chunks", [])
    lines.append(f"## ìƒ· ë¶„ì„ ì²­í¬ (ì´ {len(chunks)}ê°œ)")
    lines.append("")
    
    for chunk in chunks:
        meta = chunk.get("metadata", {})
        content = chunk.get("content", {})
        
        lines.append(f"### {meta.get('film_title', 'Unknown')} - {meta.get('temporal_phase', 'N/A')}")
        lines.append(f"- **ì²­í¬ ID**: {chunk.get('chunk_id', 'N/A')}")
        lines.append(f"- **ì”¬ ë²”ìœ„**: {meta.get('scene_range', 'N/A')}")
        lines.append(f"- **ëŒ€ì‚¬/ì„¤ëª…**: {content.get('transcript', 'N/A')}")
        lines.append("")
        
        visual = content.get("visual_schema", {})
        lines.append(f"#### ì‹œê° ë¶„ì„")
        lines.append(f"- êµ¬ë„: {visual.get('composition', 'N/A')}")
        lines.append(f"- ì¡°ëª…: {visual.get('lighting', 'N/A')}")
        lines.append(f"- ì¹´ë©”ë¼: {visual.get('camera_motion', 'N/A')}")
        lines.append(f"- í˜ì´ì‹±: {visual.get('pacing', 'N/A')}")
        lines.append("")
        
        motifs = content.get("motifs", [])
        if motifs:
            lines.append(f"#### ëª¨í‹°í”„: {', '.join(motifs)}")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    # Motif Registry
    registry = data.get("motif_registry", {})
    recurring = registry.get("recurring_motifs", [])
    if recurring:
        lines.append("## ë°˜ë³µ ëª¨í‹°í”„ ë ˆì§€ìŠ¤íŠ¸ë¦¬")
        lines.append("")
        for motif in recurring:
            lines.append(f"### {motif.get('name', 'N/A')}")
            lines.append(f"- **ì˜ë¯¸**: {motif.get('semantic_meaning', 'N/A')}")
            lines.append(f"- **ë¹ˆë„**: {motif.get('frequency', 'N/A')}")
            lines.append(f"- **ë“±ì¥**: {', '.join(motif.get('occurrences', []))}")
            lines.append("")
    
    return "\n".join(lines)


def format_layer1_as_text(data: dict) -> str:
    """Format Layer 1 JSON as readable text for NotebookLM."""
    lines = []
    lines.append("# ë´‰ì¤€í˜¸ í´ëŸ¬ìŠ¤í„° - êµ¬ì¡°í™” ì§€ì‹ (Layer 1)")
    lines.append("")
    lines.append(f"í´ëŸ¬ìŠ¤í„° ID: {data.get('cluster_id', 'N/A')}")
    lines.append(f"ê°ë…: {data.get('auteur', 'N/A')}")
    lines.append("")
    
    # Logic Vector
    logic = data.get("logic_vector", {})
    lines.append("## Logic Vector (ìˆ˜í•™ì  ë¡œì§)")
    lines.append("")
    lines.append(f"**Logic ID**: {logic.get('logic_id', 'N/A')}")
    lines.append(f"**ì„¤ëª…**: {logic.get('description', 'N/A')}")
    lines.append("")
    
    cadence = logic.get("cadence", {})
    lines.append("### ì¼€ì´ë˜ìŠ¤ (Cadence)")
    shot_len = cadence.get("shot_length_ms", {})
    lines.append(f"- ìƒ· ê¸¸ì´ ì¤‘ì•™ê°’: {shot_len.get('median', 'N/A')}ms")
    lines.append(f"- íŠ¹ì§•: {shot_len.get('signature', 'N/A')}")
    lines.append("")
    
    cut_density = cadence.get("cut_density", {})
    lines.append("### ì»· ë°€ë„ (Temporal Phaseë³„)")
    for phase, density in cut_density.items():
        lines.append(f"- {phase.upper()}: {density}")
    lines.append("")
    
    composition = logic.get("composition", {})
    lines.append("### êµ¬ë„ (Composition)")
    lines.append(f"- ì£¼ìš” ì „ëµ: {composition.get('primary_strategy', 'N/A')}")
    lines.append(f"- ëŒ€ì¹­ ì ìˆ˜: {composition.get('symmetry_score', 'N/A')}")
    lines.append(f"- ì‹¬ë„: {composition.get('depth_usage', 'N/A')}")
    sig_comp = composition.get("signature_compositions", [])
    if sig_comp:
        lines.append(f"- ì‹œê·¸ë‹ˆì²˜ êµ¬ë„: {', '.join(sig_comp)}")
    lines.append("")
    
    camera = logic.get("camera_motion", {})
    lines.append("### ì¹´ë©”ë¼ ì›€ì§ì„")
    lines.append(f"- íŠ¹ì§•: {camera.get('signature', 'N/A')}")
    lines.append(f"- ìŠ¤íƒœí‹±: {camera.get('static', 0)*100:.0f}%")
    lines.append(f"- ëŒë¦¬: {camera.get('dolly', 0)*100:.0f}%")
    lines.append(f"- í•¸ë“œí—¬ë“œ: {camera.get('handheld', 0)*100:.0f}%")
    lines.append("")
    
    # Persona Vector
    persona = data.get("persona_vector", {})
    lines.append("## Persona Vector (ì˜ˆìˆ ì  í˜ë¥´ì†Œë‚˜)")
    lines.append("")
    lines.append(f"**Persona ID**: {persona.get('persona_id', 'N/A')}")
    lines.append(f"**ì„¤ëª…**: {persona.get('description', 'N/A')}")
    lines.append("")
    
    tone = persona.get("tone", [])
    lines.append(f"### í†¤: {', '.join(tone)}")
    lines.append("")
    
    lines.append("### ê°ì • ê³¡ì„  (Emotion Arc)")
    emotion_arc = persona.get("emotion_arc", [])
    for point in emotion_arc:
        lines.append(f"- t={point.get('t', 0)}: {point.get('label', 'N/A')} (valence={point.get('valence', 0)}, arousal={point.get('arousal', 0)})")
    lines.append("")
    
    frames = persona.get("interpretation_frame", [])
    lines.append(f"### í•´ì„ í”„ë ˆì„: {', '.join(frames)}")
    lines.append("")
    
    # Pattern Rules
    rules = data.get("pattern_rules", [])
    if rules:
        lines.append("## íŒ¨í„´ ê·œì¹™")
        lines.append("")
        for rule in rules:
            lines.append(f"### {rule.get('name', 'N/A')}")
            lines.append(f"- **ì„¤ëª…**: {rule.get('description', 'N/A')}")
            lines.append(f"- **ì ìš© ì¡°ê±´**: {rule.get('application_condition', 'N/A')}")
            lines.append("")
    
    # Fusion Formula
    fusion = data.get("fusion_formula", {})
    lines.append("## Fusion ê³µì‹")
    lines.append(f"- **ê³µì‹**: `{fusion.get('formula', 'N/A')}`")
    thresholds = fusion.get("thresholds", {})
    lines.append(f"- **ë™ì¼ í´ëŸ¬ìŠ¤í„°**: D <= {thresholds.get('same_cluster', 'N/A')}")
    lines.append(f"- **í´ëŸ¬ìŠ¤í„° ë¶„ê¸°**: D >= {thresholds.get('split_cluster', 'N/A')}")
    lines.append("")
    
    return "\n".join(lines)


def export_as_text_files(sources: list, output_dir: Path):
    """Export sources as text files for manual upload."""
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for i, source in enumerate(sources):
        filename = f"{i+1:02d}_{source['title'].replace(' ', '_').replace(':', '').replace('-', '_')}.txt"
        filepath = output_dir / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"# {source['title']}\n\n")
            f.write(source['content'])
        
        print(f"âœ“ Exported: {filepath}")
    
    print(f"\nì´ {len(sources)}ê°œ íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    print(f"ìœ„ì¹˜: {output_dir}")


def upload_to_notebooklm_api(sources: list, project_id: str, notebook_title: str):
    """
    Upload sources to NotebookLM via Enterprise API.
    
    NOTE: ì´ í•¨ìˆ˜ëŠ” Enterprise API í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ëœ í™˜ê²½ì—ì„œë§Œ ì‘ë™í•©ë‹ˆë‹¤.
    í˜„ì¬ Google Cloud NotebookLM APIê°€ í™œì„±í™”ëœ í”„ë¡œì íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    """
    try:
        # Enterprise API í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸ ì‹œë„
        # ì‹¤ì œ íŒ¨í‚¤ì§€ ì´ë¦„ì€ Google Cloud ë¬¸ì„œ ì°¸ì¡° í•„ìš”
        # from google.cloud import notebooklm
        print("âš ï¸  NotebookLM Enterprise API í´ë¼ì´ì–¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   í˜„ì¬ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ë³´ë‚´ê¸°ë§Œ ì§€ì›ë©ë‹ˆë‹¤.")
        print("   ìˆ˜ë™ ì—…ë¡œë“œë¥¼ ìœ„í•´ --export-only ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return False
    except ImportError:
        print("âŒ google-cloud-notebooklm íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False


def main():
    parser = argparse.ArgumentParser(description="NotebookLM Source Pack Uploader")
    parser.add_argument("--project-id", help="Google Cloud Project ID")
    parser.add_argument("--notebook-title", default="ë´‰ì¤€í˜¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„", help="Notebook title")
    parser.add_argument("--export-only", action="store_true", help="Export as text files only")
    parser.add_argument("--output-dir", default="./notebooklm_upload", help="Output directory for text files")
    
    args = parser.parse_args()
    
    print("ğŸ“¦ Loading source pack layers...")
    layers = load_layer_files()
    print(f"   Loaded {len(layers)} layers")
    
    print("\nğŸ”„ Converting to NotebookLM format...")
    sources = convert_to_notebooklm_format(layers)
    print(f"   Prepared {len(sources)} sources")
    
    if args.export_only or not args.project_id:
        print("\nğŸ“ Exporting as text files for manual upload...")
        output_dir = Path(args.output_dir)
        export_as_text_files(sources, output_dir)
        
        print("\nğŸ“‹ ìˆ˜ë™ ì—…ë¡œë“œ ì•ˆë‚´:")
        print("   1. https://notebooklm.google.com ì ‘ì†")
        print("   2. 'ìƒˆ ë…¸íŠ¸ë¶' ìƒì„± â†’ ì œëª©: 'ë´‰ì¤€í˜¸ í´ëŸ¬ìŠ¤í„° ë¶„ì„'")
        print("   3. 'ì†ŒìŠ¤ ì¶”ê°€' â†’ 'í…ìŠ¤íŠ¸ ë¶™ì—¬ë„£ê¸°'")
        print("   4. ê° .txt íŒŒì¼ì˜ ë‚´ìš©ì„ ìˆœì„œëŒ€ë¡œ ë¶™ì—¬ë„£ê¸°")
        print("   5. ëª¨ë“  ì†ŒìŠ¤ ì¶”ê°€ í›„ 'ê°œìš” ë…¸íŠ¸' ìƒì„±")
    else:
        print("\nâ˜ï¸  Uploading to NotebookLM API...")
        success = upload_to_notebooklm_api(sources, args.project_id, args.notebook_title)
        if not success:
            print("\nğŸ’¡ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ë ¤ë©´ --export-only ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
